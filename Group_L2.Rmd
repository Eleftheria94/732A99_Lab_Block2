---
title: "732A99 Lab Block 2"
author: "Rasmus Säfvenberg, Eleftheria Chatzitheodoridou & Syed Muhammad Arslan Haider"
date: '2020-11-14'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, 
                      out.width = "100%")
library(randomForest)
library(ggplot2)
library(reshape2)
library(dplyr)
library(pamr)
```

## Assignment 1

### Question 1a)

*Repeat the procedure above for 100 training datasets and report the mean and variance of the misclassification errors. In other words, create 100 training datasets, learn a random forest from each dataset, and compute the misclassification error in the same test dataset. Report results for when the random forest has 1, 10 and 100 trees.*

```{r 1.a}
# Test
set.seed(1234)

x1 <- runif(1000)
x2 <- runif(1000)
tedata <- cbind(x1, x2)
y <- as.numeric(x1 < x2)
telabels <- as.factor(y)
plot(x1, x2, col = (y + 1))

# Train
misclass_1 <- vector("numeric", length = 100L)
misclass_10 <- vector("numeric", length = 100L)
misclass_100 <- vector("numeric", length = 100L)
for(i in 1:100){
  set.seed(i)
  x1 <- runif(100)
  x2 <- runif(100)
  trdata <- cbind(x1, x2)
  y <- as.numeric(x1 < x2)
  trlabels <- as.factor(y)
  for(num_trees in c(1, 10, 100)){
    forest <- randomForest(trlabels ~ x1 + x2, ntree = num_trees, nodesize = 25,
                           keep.forest = TRUE)
    pred <- predict(forest, tedata)
    if(num_trees == 1){
      misclass_1[i] <- (sum(telabels != pred)) / length(telabels)
    } else if(num_trees == 10){
      misclass_10[i] <- (sum(telabels != pred)) / length(telabels)
    } else{
      misclass_100[i] <- (sum(telabels != pred)) / length(telabels)
    }
  }
}

df <- melt(data.frame(misclass_1, misclass_10, misclass_100))
mean_var <-data.frame(variable = c("misclass_1", "misclass_10", "misclass_100"), 
                      mean = c(mean(misclass_1), mean(misclass_10), mean(misclass_100)),
                      variance = c(var(misclass_1), var(misclass_10), var(misclass_100)))
df %>% 
  group_by(variable) %>% 
  mutate(mean = mean(value), variance = var(value)) %>% 
  ungroup() %>% 
  ggplot(aes(variable, value * 100)) + geom_boxplot() + 
  xlab("") + ylab("Misclassification error (%)") + 
  scale_x_discrete(labels = c("1 tree", "10 trees", "100 trees")) + 
  scale_y_continuous(limits = c(0, 100)) + theme_minimal() + 
  theme(panel.grid = element_blank()) + 
  stat_summary(geom = "text", fun = max,
               aes(label = paste0("Mean error: ", sprintf("%1.3f", mean), "\n", 
                                  "Variance of error: ", sprintf("%1.3f", variance))),
               position = position_nudge(y = 10), size = 3.5) 


```

### Question 1b)

*b.	Repeat the exercise above but this time use the condition (x1<0.5) instead of (x1<x2) when producing the training and test datasets.*

```{r 1.b}
# Test
set.seed(1234)

x1 <- runif(1000)
x2 <- runif(1000)
tedata <- cbind(x1, x2)
y <- as.numeric(x1 < 0.5)
telabels <- as.factor(y)
plot(x1, x2, col = (y + 1))

# Train
misclass_1 <- vector("numeric", length = 100L)
misclass_10 <- vector("numeric", length = 100L)
misclass_100 <- vector("numeric", length = 100L)
for(i in 1:100){
  set.seed(i)
  x1 <- runif(100)
  x2 <- runif(100)
  trdata <- cbind(x1, x2)
  y <- as.numeric(x1 < 0.5)
  trlabels <- as.factor(y)
  for(num_trees in c(1, 10, 100)){
    forest <- randomForest(trlabels ~ x1 + x2, ntree = num_trees, nodesize = 25, 
                           keep.forest = TRUE)
    pred <- predict(forest, tedata)
    if(num_trees == 1){
      misclass_1[i] <- (sum(telabels != pred)) / length(telabels)
    } else if(num_trees == 10){
      misclass_10[i] <- (sum(telabels != pred)) / length(telabels)
    } else{
      misclass_100[i] <- (sum(telabels != pred)) / length(telabels)
    }
  }
}

df <- melt(data.frame(misclass_1, misclass_10, misclass_100))
df %>% 
  group_by(variable) %>% 
  mutate(mean = mean(value), variance = var(value)) %>% 
  ungroup() %>% 
  ggplot(aes(variable, value * 100)) + geom_boxplot() + 
  xlab("") + ylab("Misclassification error (%)") + 
  scale_x_discrete(labels = c("1 tree", "10 trees", "100 trees")) + 
  scale_y_continuous(limits = c(0, 100)) + theme_minimal() + 
  theme(panel.grid = element_blank()) + 
  stat_summary(geom = "text", fun = max,
               aes(label = paste0("Mean error: ", sprintf("%1.3f", mean), "\n", 
                                  "Variance of error: ", sprintf("%1.3f", variance))),
               position = position_nudge(y = 10), size = 3.5) 

```

### Question 1c)

*c.	Repeat the exercise above but this time use the condition ((x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5)) instead of (x1<x2) when producing the training and test datasets. Unlike above, use nodesize = 12 for this exercise.*

```{r 1.c}
# Test
set.seed(1234)

x1 <- runif(1000)
x2 <- runif(1000)
tedata <- cbind(x1, x2)
y <- as.numeric(((x1 < 0.5 & x2 < 0.5) | (x1 > 0.5 & x2 > 0.5)) )
telabels <- as.factor(y)
plot(x1, x2, col = (y + 1))

# Train
misclass_1 <- vector("numeric", length = 100L)
misclass_10 <- vector("numeric", length = 100L)
misclass_100 <- vector("numeric", length = 100L)
for(i in 1:100){
  set.seed(i)
  x1 <- runif(100)
  x2 <- runif(100)
  trdata <- cbind(x1, x2)
  y <- as.numeric(((x1 < 0.5 & x2 < 0.5) | (x1 > 0.5 & x2 > 0.5)) )
  trlabels <- as.factor(y)
  for(num_trees in c(1, 10, 100)){
    forest <- randomForest(trlabels ~ x1 + x2, ntree = num_trees, nodesize = 12, 
                           keep.forest = TRUE)
    pred <- predict(forest, tedata)
    if(num_trees == 1){
      misclass_1[i] <- (sum(telabels != pred)) / length(telabels)
    } else if(num_trees == 10){
      misclass_10[i] <- (sum(telabels != pred)) / length(telabels)
    } else{
      misclass_100[i] <- (sum(telabels != pred)) / length(telabels)
    }
  }
}

df <- melt(data.frame(misclass_1, misclass_10, misclass_100))
df %>% 
  group_by(variable) %>% 
  mutate(mean = mean(value), variance = var(value)) %>% 
  ungroup() %>% 
  ggplot(aes(variable, value * 100)) + geom_boxplot() + 
  xlab("") + ylab("Misclassification error (%)") + 
  scale_x_discrete(labels = c("1 tree", "10 trees", "100 trees")) + 
  scale_y_continuous(limits = c(0, 100)) + theme_minimal() + 
  theme(panel.grid = element_blank()) + 
  stat_summary(geom = "text", fun = max,
               aes(label = paste0("Mean error: ", sprintf("%1.3f", mean), "\n", 
                                  "Variance of error: ", sprintf("%1.3f", variance))),
               position = position_nudge(y = 10), size = 3.5) 

```

### Question 1d)

* a) *What happens with the mean and variance of the error rate when the number of trees in the random forest grows ?*
Across all three scenarios, it can be seen that the mean error rate decreases as
the number of trees increases, which can also be seen for the variance of the error,
except in the first case, where the variance of 10 trees is slightly lower than
that of 100 trees (6 decimal places). The reason the mean error rate is decreasing
with increasing amount of trees can be explained by the majority voting principle, 
where more trees will, on average, predict better than an average individual tree. 
The same should also be applied to the variance to an extent, but the variance will
eventually increase with larger forests as it overfits the data, which leads to 
poor generalizability on new data. This phenomenon is known as the bias-variance
trade-off. 

* b) *The third dataset represents a slightly more complicated classification problem than the first one. Still, you should get better performance for it when using sufficient trees in the random forest. Explain why you get better performance.*
The first classification problem has, as observed from the graphs, a linear 
decision boundary with two distinct class, which is a simple problem in nature.
However, it can require many trees in a random forest to classify accurately due
to the large amount of splits needed to partition the feature space into distinct
classification regions, i.e., hypercubes. 

On the contrary, the second classification problem has four distinct regions, of
which the diagonal elements belong to the same class. This problem should have
better performance than the aforementioned problem, since it requires less partitions
of the feature space in order to achieve accurate classification.



* c) *Why is it desirable to have low error variance?*
Having low error variance is desirable because it reduces the variability of the 
estimated error, thus leading to more accurate estimations around the mean. A
high error variance can be thought of having bad aim in a game of darts; the darts
end up all over the place, whereas low variance has the darts centered in a specific
region. 


## Assignment 2

Your task is to implement the EM algorithm for mixtures of multivariate Bernoulli distributions. Please use the R template below to solve the assignment. Then, use your implementation to show what happens when your mixture model has too few and too many components, i.e. set K=2,3,4 and compare results. Please provide a short explanation as well.


```{r 2}

set.seed(1234567890)

max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data

true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}

K=3 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu

for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)

# E-step: Computation of the fractional component assignments
# Your code here

#Log likelihood computation.
# Your code here

cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console() 
# Stop if the lok likelihood has not changed significantly
# Your code here

#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
}
pi

mu
plot(llik[1:it], type="o")


```

## Assignment 3: High-Dimensional Methods

*Data file* **geneexp.csv** *contains information about gene expression of three different cell types (column Cell Type). These cell types are CD4 and CD8 (two sorts of T cells) and CD19 (B cells). The aim of this assignment is to classify cells to the appropriate cell types using gene expressions and discover relevant genes for the given cell types.*

*1. Divide data into training and test sets (70/30) without scaling. Perform* **nearest shrunken centroid classification** *of training data in which the threshold is chosen by cross-validation. Provide a* **centroid plot** *and interpret it. How many genes were selected by the method? What meaning do positive and negative values have in the centroid plot? Can it happen that all values in the centroid plot are positive for some gene?*

```{r 3.1}
# NSC "shrinks" each of the class centroids towardS the overall centroid for all classes by a  threshold. This shrinkage makes the classifier more accurate by eliminating the effect of noisy genes and as a result it does automatic gene selection. The gene expression profile of a new sample is compared to  each of these class centroids. The class whose centroid that it is closest to, in squared distance, is the predicted class for that new sample. This part is the same as the usual nearest centroid rule. This type of classifier is sensitive to a small disturbance and performance is inferior to contemporary classifiers such as neural networks, support vector machines, and decision trees where  complex criteria is used for classification. 

# Read data
gene <- read.csv("geneexp.csv", header =  TRUE, row.names = 1, sep = ",")

# Data pre-processing
CD4 <- gene[grep('^CD4', rownames(gene)),] #T cells
CD8 <- gene[grep('^CD8', rownames(gene)),] # T cells
CD19 <- gene[grep('^CD19', rownames(gene)),] # B cells

CD4_train <- train[grep('^CD4', rownames(train))]
CD8_train <- train[grep('^CD8', rownames(train))]
CD19_train <- train[grep('^CD19', rownames(train))]

CD4_test <- test[grep('^CD4', rownames(test))]
CD8_test <- test[grep('^CD8', rownames(test))]
CD19_test <- test[grep('^CD19', rownames(test))]

# Divide data into training and test(70/30, no scaling)
n <- dim(gene)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.7))
train <- gene[id,]
test <- gene[-id,]

# train
#rownames(train) <- 1:nrow(train)
x_train <- t(train[,unlist(-CD19_train)]) # remove dependent variable
y_train <- train[[unlist(CD19_train)]] # vector of the dependent variable
mygene_train <- list(x = x_train, y = as.factor(y_train), geneid = as.character(1:nrow(x_train)), genenames = rownames(x_train))

# test
#rownames(test) <- 1:nrow(test)
x_test <- t(test[,-CD19_test])
y_test <- test[[CD19_test]]
mygene_test <- list(x = x_test, y = as.factor(y_test), geneid = as.character(1:nrow(x_test)), genenames = rownames(x_test))

# Create model
model <- pamr.train(mygene_train, threshold = seq(0, 4, 0.1))

# Choose threshold by cross-validation
cvmodel <- pamr.cv(model, mygene_train)

# Misclassification Error plot
pamr.plotcv(cvmodel)

# Print cvmodel
print(cvmodel)

```

*2.	List the names of the 2 most contributing genes and find their alternative names in Google. Then, by checking this webpage <https://panglaodb.se/markers.html> find out whether these two genes are “marker genes” for given cell types. Report the test error of the model.*

```{r 3.2}
```

*3.	Compute the test error and the number of the contributing features for the following methods fitted to the training data:*

   a. *Elastic net with the binomial response and* $\alpha = 0.5$ *in which penalty is selected by the cross-validation.*
   
   b. *Support vector machine with “vanilladot” kernel.*
   
*Compare the results of these models with the results of the nearest shrunken centroids (make a comparative table). Which model would you prefer and why?*

```{r 3.3}
```

*4. Implement Benjamini-Hochberg method for the original data in which you test each cell type versus the remaining ones, and use `t.test()` for computing p-values. Present plots showing p-values and the rejection area for each cell type and interpret them. How many genes correspond to the rejected hypotheses for each cell type?*

```{r 3.4}
```